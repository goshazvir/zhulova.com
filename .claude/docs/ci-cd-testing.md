# CI/CD Testing Strategy

This document describes the automated testing strategy implemented in GitHub Actions following industry best practices.

## Table of Contents

- [Overview](#overview)
- [Workflow Architecture](#workflow-architecture)
- [Sequential Execution Pattern](#sequential-execution-pattern)
- [Optimization Strategy](#optimization-strategy)
- [Artifact Management](#artifact-management)
- [Performance Metrics](#performance-metrics)
- [Alternative Approaches Comparison](#alternative-approaches-comparison)
- [Practical Examples](#practical-examples)
- [Local Development](#local-development)
- [Troubleshooting](#troubleshooting)

---

## Overview

**Implementation Date:** 2024-11-24
**Workflow File:** `.github/workflows/test.yml`
**Pattern:** Sequential Jobs with Fast-Fail
**Test Coverage:** 78 unit tests + 73 E2E tests

### Key Features

- âœ… **Fast-fail pattern** - E2E tests skip if unit tests fail (saves 3-4 minutes per failed build)
- âœ… **Optimized CI execution** - Only Chromium browser on CI (4x faster than all browsers)
- âœ… **Smart artifact retention** - Different retention policies for different artifact types
- âœ… **Automated PR comments** - Coverage and test results automatically posted to PRs
- âœ… **Cost-efficient** - 77% reduction in CI minutes (15.5 min â†’ 3.5 min)

---

## Workflow Architecture

### Visual Flow

```
PR Created/Updated
        â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                 â”‚                 â”‚
   [test.yml]    [performance-gate.yml]  (parallel)
        â”‚                 â”‚
        â”‚                 â”œâ”€ Lighthouse CI
        â”‚                 â”œâ”€ Bundle Size
        â”‚                 â””â”€ Comment PR âœ…
        â”‚
   â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
   â”‚ Job 1:  â”‚  Unit Tests (Vitest)
   â”‚ ~30s    â”‚  âœ“ 78 tests
   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  âœ“ Coverage report
        â”‚       âœ“ Comment PR
        â”‚
        â”œâ”€ FAIL? âŒ â†’ Stop (E2E skipped, saves 3-4 min)
        â”‚
        â””â”€ PASS? âœ… â†’ Continue
                â”‚
           â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
           â”‚ Job 2:  â”‚  E2E Tests (Playwright)
           â”‚ ~3-4min â”‚  âœ“ 73 tests (Chromium only)
           â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  âœ“ HTML report (30d retention)
                â”‚       âœ“ Screenshots on failure (7d)
                â”‚
           â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
           â”‚ Job 3:  â”‚  Test Summary
           â”‚ always  â”‚  âœ“ Overall status
           â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  âœ“ Fail if any job failed
                â”‚
                â””â”€ All tests passed âœ…
```

### Three Jobs

#### Job 1: Unit Tests (Vitest)
```yaml
- Runs: Always (first)
- Duration: ~30 seconds
- Tests: 78 unit tests
- Coverage: Generates json-summary for parsing
- Artifacts: coverage-report (14 days retention)
- PR Comment: Coverage table with percentages
```

#### Job 2: E2E Tests (Playwright)
```yaml
- Runs: Only if unit tests pass (needs: unit-tests)
- Duration: ~3-4 minutes
- Tests: 73 E2E tests
- Browser: Chromium only on CI (Chrome, Firefox, Safari, Mobile locally)
- Artifacts:
  - playwright-report (30 days, always)
  - test-results (7 days, only on failure)
- PR Comment: Test results table with pass/fail counts
```

#### Job 3: Test Summary
```yaml
- Runs: Always (if: always())
- Checks: Both unit and E2E job results
- Action: Fails workflow if any test suite failed
- Purpose: Clear overall status indicator
```

---

## Sequential Execution Pattern

### Why Sequential (Unit â†’ E2E)?

**1. Cost Efficiency** ğŸ’°
```
Unit tests: cheap (seconds)
E2E tests: expensive (minutes)
Don't run E2E if unit tests fail â†’ saves 3-4 minutes per failed build
```

**2. Fast Feedback** ğŸš€
```
Unit test failures reported in ~30 seconds
Developers can fix quickly without waiting for E2E
```

**3. Logical Flow** ğŸ§©
```
If basic units are broken, integration tests will fail anyway
Fix foundations first, then test integration
```

### Fast-Fail Pattern

```bash
# Scenario A: Unit tests fail
Unit Tests (30s) â†’ FAIL âŒ
  â†’ E2E Tests: SKIPPED
  â†’ Total time: 30 seconds
  â†’ CI minutes saved: 3-4 minutes

# Scenario B: Unit tests pass, E2E tests run
Unit Tests (30s) â†’ PASS âœ…
  â†’ E2E Tests (3-4 min) â†’ PASS âœ…
  â†’ Total time: 3.5-4.5 minutes

# Scenario C: E2E tests fail
Unit Tests (30s) â†’ PASS âœ…
  â†’ E2E Tests (3-4 min) â†’ FAIL âŒ
  â†’ Artifacts uploaded (screenshots, traces)
  â†’ Total time: 3.5-4.5 minutes
```

### Cost Savings Analysis

**Monthly Build Statistics (estimated):**
- PRs per month: ~40
- Failed builds (unit): ~20 (50%)
- Failed builds (E2E): ~5 (12.5%)
- Successful builds: ~15 (37.5%)

**WITHOUT Fast-Fail Pattern:**
```
All builds run full 15.5 minutes
Total CI minutes: 40 Ã— 15.5 = 620 minutes/month
```

**WITH Fast-Fail Pattern:**
```
Unit failures: 20 Ã— 0.5 min = 10 minutes
E2E failures:  5 Ã— 4 min = 20 minutes
Successful:    15 Ã— 4 min = 60 minutes
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total CI minutes: 90 minutes/month

Savings: 620 - 90 = 530 minutes/month (85% reduction)
```

---

## Optimization Strategy

### Playwright Browser Selection

#### On CI (process.env.CI === true)
```typescript
projects: [
  {
    name: 'chromium',
    use: { ...devices['Desktop Chrome'] },
  },
]
```

**Rationale:**
- âœ… 90% of users use Chromium-based browsers (Chrome, Edge, Opera, Brave)
- âœ… Fastest execution (1 browser vs 5 browsers)
- âœ… Most critical path coverage
- âœ… Can add Firefox/Safari as scheduled nightly runs if needed

#### Locally (development)
```typescript
projects: [
  { name: 'chromium', use: { ...devices['Desktop Chrome'] } },
  { name: 'firefox', use: { ...devices['Desktop Firefox'] } },
  { name: 'webkit', use: { ...devices['Desktop Safari'] } },
  { name: 'Mobile Chrome', use: { ...devices['Pixel 5'] } },
  { name: 'Mobile Safari', use: { ...devices['iPhone 12'] } },
]
```

**Rationale:**
- âœ… Comprehensive browser coverage during development
- âœ… Catch browser-specific issues early
- âœ… Developers can test specific browsers with `npx playwright test --project=firefox`

### Performance Comparison

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘           Playwright CI Optimization Comparison                â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

BEFORE (all 5 projects):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â€¢ Chromium (Desktop Chrome)      ~3 min                     â”‚
â”‚ â€¢ Firefox (Desktop Firefox)      ~3 min                     â”‚
â”‚ â€¢ Webkit (Desktop Safari)        ~3 min                     â”‚
â”‚ â€¢ Mobile Chrome (Pixel 5)        ~3 min                     â”‚
â”‚ â€¢ Mobile Safari (iPhone 12)      ~3 min                     â”‚
â”‚                                  â”€â”€â”€â”€â”€                       â”‚
â”‚ Total E2E time:                  15 min                     â”‚
â”‚ + Unit tests:                    +30s                       â”‚
â”‚ = Total CI time:                 ~15.5 min                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

AFTER (Chromium only):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â€¢ Chromium (Desktop Chrome)      ~3 min                     â”‚
â”‚ â€¢ Skipped (Firefox, Safari, etc) saved 12 min              â”‚
â”‚                                  â”€â”€â”€â”€â”€                       â”‚
â”‚ Total E2E time:                  3 min                      â”‚
â”‚ + Unit tests:                    +30s                       â”‚
â”‚ = Total CI time:                 ~3.5 min                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

SAVINGS: 15.5 min â†’ 3.5 min (77% faster, 4.4x speedup)
```

### Reporter Configuration

#### Vitest (Unit Tests)
```typescript
coverage: {
  provider: 'v8',
  reporter: ['text', 'json', 'json-summary', 'html'],
  //                         ^^^^^^^^^^^^^ Added for GitHub Actions parsing
}
```

**Purpose:**
- `json-summary` generates `coverage/coverage-summary.json`
- GitHub Actions parses this file to post PR comments
- Contains: `{ statements, branches, functions, lines }` percentages

#### Playwright (E2E Tests)
```typescript
reporter: process.env.CI
  ? [
      ['html'],  // Visual report
      ['json', { outputFile: 'test-results/.last-run.json' }],  // For parsing
      ['list'],  // Terminal output
    ]
  : 'html'  // Locally just HTML
```

**Purpose:**
- `json` reporter generates parseable test results
- GitHub Actions reads `test-results/.last-run.json`
- Contains: `{ passed, failed, flaky, skipped }` counts

---

## Artifact Management

### Retention Policies

#### Coverage Reports (14 days)
```yaml
- name: Upload coverage report
  if: always()
  uses: actions/upload-artifact@v4
  with:
    name: coverage-report
    path: coverage/
    retention-days: 14
```

**Rationale:**
- âœ… Medium-term trend analysis
- âœ… Compare coverage changes over sprints
- âœ… Not too short (lost historical data) or too long (storage costs)

#### Playwright HTML Reports (30 days)
```yaml
- name: Upload Playwright HTML report
  if: always()
  uses: actions/upload-artifact@v4
  with:
    name: playwright-report
    path: playwright-report/
    retention-days: 30
```

**Rationale:**
- âœ… Long-term historical comparison
- âœ… Useful for identifying test flakiness trends
- âœ… Small file size (HTML + JSON, ~1-2MB)

#### Screenshots & Traces (7 days)
```yaml
- name: Upload test screenshots and traces
  if: failure()  # Only when tests fail
  uses: actions/upload-artifact@v4
  with:
    name: test-results
    path: test-results/
    retention-days: 7
```

**Rationale:**
- âœ… Short retention (only recent failures matter)
- âœ… Large file size (PNG images, traces can be 10-50MB)
- âœ… Only uploaded on failure (saves storage)
- âœ… 7 days is enough to debug and fix

### Artifact Access

**In GitHub UI:**
1. Go to **Actions** tab
2. Click on specific workflow run
3. Scroll to **Artifacts** section at bottom
4. Download artifact ZIP file

**Artifacts Available:**
- `coverage-report` - HTML + JSON coverage data
- `playwright-report` - Interactive HTML test report
- `test-results` - Screenshots, traces, videos (only on failure)

---

## Performance Metrics

### Key Metrics Tracked

| Metric | Current Value | Target | Status |
|--------|--------------|--------|--------|
| **Unit Test Pass Rate** | ~95% | 95%+ | âœ… |
| **E2E Test Pass Rate** | ~90% | 95%+ | ğŸŸ¡ |
| **Code Coverage (Statements)** | 40% | 80% | ğŸŸ¡ |
| **Code Coverage (Branches)** | 40% | 80% | ğŸŸ¡ |
| **Lighthouse Performance** | 95+ | 85+ | âœ… |
| **JavaScript Bundle Size** | 0KB | <100KB | âœ… |
| **CSS Bundle Size** | 12KB | <20KB | âœ… |
| **CI Duration (Success)** | 3.5 min | <5 min | âœ… |
| **CI Duration (Unit Fail)** | 30s | <1 min | âœ… |

### Monitoring Strategy

**Where to View:**
- **Test results**: GitHub Actions â†’ Workflow run â†’ Summary
- **Coverage trends**: Download artifacts over time, compare coverage-summary.json
- **Performance**: Vercel Dashboard â†’ Speed Insights
- **E2E failures**: Download playwright-report artifact, open index.html

**Alerting:**
- Test failures â†’ GitHub Actions email notifications
- Performance degradation â†’ `performance-alerts.yml` workflow creates issues
- Coverage drop â†’ Manual review of PR comments

---

## Alternative Approaches Comparison

### Option 1: Single Workflow (One Job)

```yaml
jobs:
  test:
    - npm run test:run        # Unit
    - npm run test:e2e        # E2E
    - upload artifacts
```

**Pros:**
- âœ… Simplest approach
- âœ… One file configuration

**Cons:**
- âŒ Slow feedback (5-10 min)
- âŒ Unit failures don't stop E2E â†’ waste CI minutes
- âŒ Hard to re-run only E2E tests
- âŒ All 5 Playwright projects run â†’ very slow

**Verdict:** ğŸ”´ **NOT RECOMMENDED** - Inefficient for CI

---

### Option 2: Separate Workflows (unit.yml + e2e.yml)

```yaml
# unit-tests.yml
jobs:
  unit:
    - npm run test:run

# e2e-tests.yml
jobs:
  e2e:
    - npm run test:e2e
```

**Pros:**
- âœ… Fast feedback (unit tests ~30s)
- âœ… Can re-run separately
- âœ… Clear separation of concerns

**Cons:**
- âŒ Two separate workflows â†’ duplicate setup code
- âŒ E2E run even if unit tests fail
- âŒ Harder to manage (two files)
- âŒ No dependency between workflows

**Verdict:** ğŸŸ¡ **ACCEPTABLE** - But not optimal

---

### Option 3: Single Workflow + Sequential Jobs (CURRENT) âœ…

```yaml
jobs:
  unit-tests:
    - npm run test:run

  e2e-tests:
    needs: unit-tests  # Runs only if unit pass
    - npm run test:e2e

  test-summary:
    needs: [unit-tests, e2e-tests]
    if: always()
    - check overall status
```

**Pros:**
- âœ… **Fast-fail pattern** - E2E skip if unit fail â†’ saves CI minutes
- âœ… Fast feedback for unit (~30s)
- âœ… Logical sequence (unit â†’ E2E)
- âœ… One workflow file
- âœ… Can re-run individual jobs
- âœ… Different artifact retention policies

**Cons:**
- âš ï¸ Sequential (not parallel) - but this is intentional!

**Verdict:** ğŸŸ¢ **RECOMMENDED** - Best balance of speed, cost, and maintainability

---

### Option 4: Matrix Strategy

```yaml
jobs:
  test:
    strategy:
      matrix:
        test-type: [unit, e2e-chromium, e2e-firefox, e2e-safari]
    - run tests based on matrix
```

**Pros:**
- âœ… Maximum parallelization
- âœ… Easy to add new test types

**Cons:**
- âŒ Complex configuration
- âŒ Overkill for 7 unit + 4 E2E files
- âŒ E2E run even if unit fail
- âŒ All browsers tested (expensive)

**Verdict:** ğŸ”´ **NOT NEEDED** - Too complex for this project scale

---

## Practical Examples

### Scenario 1: All Tests Pass âœ…

```bash
# Developer workflow
1. Create feature branch
2. Write code + tests
3. Run locally: npm run test:run && npm run test:e2e
4. Push to GitHub
5. Create PR

# GitHub Actions
1. Unit Tests â†’ 30s â†’ âœ… PASS
   - GitHub bot comments:
     ğŸ“Š Coverage: Statements 65%, Branches 60%

2. E2E Tests â†’ 3-4 min â†’ âœ… PASS
   - GitHub bot comments:
     ğŸ§ª E2E Results: 73 passed, 0 failed

3. Test Summary â†’ âœ… PASS
   - All checks green

4. performance-gate.yml â†’ âœ… PASS
   - Lighthouse 95+, Bundle < 100KB

5. PR ready to merge âœ…

Total time: ~4 minutes
```

### Scenario 2: Unit Tests Fail âŒ

```bash
# Developer workflow
1. Push code with broken unit test
2. Create PR

# GitHub Actions
1. Unit Tests â†’ 30s â†’ âŒ FAIL
   - GitHub bot comments:
     âŒ Unit Tests Failed
     - Button.test.tsx: "should render with correct variant" failed
     - Expected: "bg-gold-500"
     - Received: "bg-navy-900"

2. E2E Tests â†’ SKIPPED â­ï¸
   - Reason: needs: unit-tests (not met)
   - CI minutes saved: 3-4 minutes

3. Test Summary â†’ âŒ FAIL
   - PR blocked from merging

# Developer fix
1. Fix the test
2. Push again
3. Only 30 seconds to get feedback

Total time: 30 seconds (not 15 minutes!)
```

### Scenario 3: E2E Tests Fail âŒ

```bash
# Developer workflow
1. Push code that breaks form submission
2. Create PR

# GitHub Actions
1. Unit Tests â†’ 30s â†’ âœ… PASS
   - Coverage looks good

2. E2E Tests â†’ 3-4 min â†’ âŒ FAIL
   - consultation-form.spec.ts: "should submit form successfully" failed
   - GitHub bot comments:
     âŒ E2E Tests: 72 passed, 1 failed
     âš ï¸ Screenshots and traces available in artifacts

3. Artifacts uploaded:
   - playwright-report/ (30 days)
   - test-results/ (7 days)
     - screenshots/consultation-form.spec.ts-failed.png
     - traces/consultation-form.spec.ts.zip

# Developer debug
1. Download test-results artifact
2. Open screenshot â†’ see error message
3. Open trace in Playwright UI â†’ see network request failed
4. Fix API endpoint
5. Push again

Total time: ~4 minutes to identify issue
```

### Scenario 4: Flaky Test (Retry Success)

```bash
# GitHub Actions
1. E2E Tests â†’ First attempt â†’ âŒ FAIL
   - "Element not found" (timing issue)

2. Playwright automatic retry (retries: 2 on CI)
   - Retry 1 â†’ âŒ FAIL
   - Retry 2 â†’ âœ… PASS

3. Test marked as "flaky" in report
   - Warning: This test passed on retry
   - Consider adding explicit waits

4. Overall status: âœ… PASS (with flaky warning)

Note: Flaky tests pass workflow but should be fixed to remove flakiness
```

---

## Local Development

### Running Tests Locally

#### Unit Tests (Vitest)
```bash
# Watch mode (recommended for development)
npm run test

# Single run (what CI uses)
npm run test:run

# With coverage report
npm run test:coverage

# View coverage report in browser
open coverage/index.html

# Interactive UI mode
npm run test:ui
```

#### E2E Tests (Playwright)

```bash
# All browsers (5 projects: Chrome, Firefox, Safari, Mobile Chrome, Mobile Safari)
npm run test:e2e

# Interactive UI mode (recommended)
npm run test:e2e:ui

# Specific browser only
npx playwright test --project=chromium
npx playwright test --project=firefox
npx playwright test --project=webkit

# View last report
npm run test:e2e:report

# Headed mode (see browser)
npx playwright test --headed

# Debug mode
npx playwright test --debug
```

### Simulating CI Environment

```bash
# Set CI environment variable
export CI=true

# Run unit tests (same as CI)
npm run test:run

# Run E2E tests (Chromium only, same as CI)
npx playwright test --project=chromium

# Check reporters work correctly
cat coverage/coverage-summary.json
cat test-results/.last-run.json

# View artifacts
open coverage/index.html
open playwright-report/index.html

# Clean up
unset CI
```

### Pre-Push Checklist

```bash
# Before pushing to GitHub, run locally:

# 1. Run all unit tests
npm run test:run
# Expected: All 78 tests pass

# 2. Check coverage thresholds
npm run test:coverage
# Expected: Statements 40%+, Branches 40%+

# 3. Run E2E tests (at least Chromium)
npx playwright test --project=chromium
# Expected: All 73 tests pass

# 4. Check TypeScript compilation
npm run build
# Expected: No type errors

# 5. If all pass, push to GitHub
git push origin feature-branch
```

---

## Troubleshooting

### "Unit tests passed locally but failed on CI"

**Possible causes:**
- Node.js version mismatch (CI uses 20.x)
- Missing environment variables
- Timezone-dependent tests
- File system differences (case sensitivity)

**Solutions:**
```bash
# 1. Check Node.js version
node -v
# If not 20.x, install: nvm install 20

# 2. Check environment variables
cat .env.example
# Verify all required vars are in GitHub Secrets

# 3. Run tests with CI=true locally
CI=true npm run test:run

# 4. Check for timezone issues
TZ=UTC npm run test:run
```

---

### "E2E tests flaky on CI"

**Symptoms:**
- Tests pass locally, fail on CI
- Tests fail randomly (pass on retry)
- "Element not found" errors

**Solutions:**
```typescript
// 1. Add explicit waits (not arbitrary timeouts)
await expect(element).toBeVisible({ timeout: 10000 });

// 2. Wait for network to be idle
await page.goto('/', { waitUntil: 'networkidle' });

// 3. Use better selectors (avoid text-based)
// Bad:
await page.click('button:has-text("Submit")');

// Good:
await page.getByRole('button', { name: /submit/i }).click();

// 4. Check Playwright retry config
// Already set: retries: process.env.CI ? 2 : 0
```

**Debug flaky test:**
```bash
# 1. Download trace artifact from CI
# GitHub Actions â†’ Failed run â†’ Artifacts â†’ test-results

# 2. View trace locally
npx playwright show-trace test-results/traces/flaky-test.zip

# 3. See exactly what happened:
#    - Network requests
#    - DOM snapshots
#    - Console logs
#    - Timing information
```

---

### "CI is too slow"

**Expected times:**
- Unit tests: ~30 seconds
- E2E tests: ~3-4 minutes
- Total (success): ~3.5-4.5 minutes

**If slower:**

```bash
# 1. Check if all 5 browsers are running on CI
# In playwright.config.ts, verify:
projects: process.env.CI
  ? [{ name: 'chromium', ... }]  # Should be ONLY Chromium
  : [/* all 5 browsers */]

# 2. Check parallelization setting
workers: process.env.CI ? 1 : undefined  # Should be 1 on CI (intentional)

# 3. Check npm cache is working
# In test.yml, verify:
- uses: actions/setup-node@v4
  with:
    cache: 'npm'  # Should be present
```

---

### "Artifacts not found"

**Issue:** Can't download coverage or playwright report

**Solutions:**

```yaml
# 1. Check if job completed
# If job was cancelled, artifacts don't upload

# 2. Check retention period
# - Coverage: 14 days
# - Playwright report: 30 days
# - Screenshots: 7 days (only on failure)

# 3. Check if condition met
- name: Upload coverage
  if: always()  # Uploads even on failure

- name: Upload screenshots
  if: failure()  # Only uploads on failure

# 4. Verify artifact name
# Artifact name in test.yml:
#   - coverage-report
#   - playwright-report
#   - test-results
```

**Where to find artifacts:**
1. GitHub â†’ Actions tab
2. Click specific workflow run
3. Scroll to bottom â†’ "Artifacts" section
4. Click artifact name to download ZIP

---

### "Coverage not showing in PR comment"

**Possible causes:**
- coverage-summary.json not generated
- Vitest configuration missing json-summary reporter
- GitHub Actions script error

**Solutions:**

```bash
# 1. Verify json-summary reporter in vitest.config.ts
coverage: {
  reporter: ['text', 'json', 'json-summary', 'html'],
  //                         ^^^^^^^^^^^^^ Must be present
}

# 2. Run locally and check file exists
npm run test:coverage
cat coverage/coverage-summary.json

# Expected output:
# {
#   "total": {
#     "statements": { "pct": 65.5 },
#     "branches": { "pct": 60.2 },
#     ...
#   }
# }

# 3. Check GitHub Actions logs
# Actions â†’ test.yml â†’ unit-tests job â†’ "Comment coverage summary"
# Look for parsing errors
```

---

### "E2E tests can't connect to server"

**Symptoms:**
- Tests fail with "Connection refused"
- `webServer.url` unreachable

**Solutions:**

```bash
# 1. Check webServer config in playwright.config.ts
webServer: {
  command: 'npm run dev',
  url: 'http://localhost:4321',
  reuseExistingServer: !process.env.CI,  # On CI, always start fresh
  timeout: 120 * 1000,  # 2 minutes to start
}

# 2. Check if port is already in use locally
lsof -i :4321
# If output shows process, kill it: kill -9 <PID>

# 3. On CI, check build step completed
# In test.yml, ensure:
- name: Build project
  run: npm run build

# 4. Check environment variables
# E2E tests might need:
- env:
    SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
    SUPABASE_ANON_KEY: ${{ secrets.SUPABASE_ANON_KEY }}
```

---

## Future Improvements

### Short-term (1-3 months)

- [ ] **Increase coverage threshold**
  - Current: 40% (statements, branches, functions, lines)
  - Target: 60% â†’ 80% incrementally
  - Update in `vitest.config.ts`: `thresholds: { lines: 60 }`

- [ ] **Add visual regression testing**
  - Use Playwright screenshots
  - Compare against baseline images
  - Alert on unexpected UI changes

- [ ] **Flaky test detection**
  - Track tests that pass on retry
  - Create GitHub issue for flaky tests
  - Goal: 0 flaky tests

### Medium-term (3-6 months)

- [ ] **Integrate Vercel Speed Insights API**
  - Fetch real user metrics in CI
  - Compare synthetic (Lighthouse) vs RUM data
  - Alert on performance regressions

- [ ] **Test result trends visualization**
  - Store test results in JSON over time
  - Generate graphs: pass rate, duration, flakiness
  - Display in GitHub Pages

- [ ] **Cross-browser testing schedule**
  - Nightly cron job: test Firefox + Safari
  - Weekly: test Mobile Chrome + Mobile Safari
  - Report issues as GitHub Issues

### Long-term (6-12 months)

- [ ] **Slack/Discord notifications**
  - Send alerts on test failures
  - Include screenshot preview
  - Link to artifacts

- [ ] **Test parallelization on CI**
  - Currently `workers: 1` (sequential)
  - Experiment with `workers: 2` if CI faster
  - Monitor for flakiness increase

- [ ] **Mutation testing**
  - Use Stryker Mutator
  - Verify test quality (not just coverage)
  - Goal: 80% mutation score

---

## Summary

### What We Achieved

âœ… **Fast-fail pattern** - E2E tests skip if unit fail â†’ 85% CI minutes savings
âœ… **Optimized execution** - Chromium only on CI â†’ 77% faster (15.5 â†’ 3.5 min)
âœ… **Smart artifacts** - Different retention policies â†’ cost-efficient storage
âœ… **Automated PR comments** - Coverage and test results â†’ great developer experience
âœ… **Comprehensive documentation** - This document + workflow README â†’ easy to maintain

### Best Practices Followed

âœ… **Industry standard pattern** - Sequential jobs with needs dependency
âœ… **Cost-conscious** - Fast-fail saves expensive CI minutes
âœ… **Developer-friendly** - Fast feedback, clear error messages
âœ… **Maintainable** - Well-documented, easy to understand
âœ… **Scalable** - Can add more test types or browsers easily

### Key Takeaways

1. **Sequential is better than parallel** when later jobs depend on earlier ones
2. **Fast-fail saves money** - Don't run expensive tests if cheap ones fail
3. **Optimize for the common case** - 90% users on Chromium â†’ test Chromium on CI
4. **Smart retention policies** - Keep what you need, delete what you don't
5. **Automate feedback** - PR comments reduce manual checking

---

**Last Updated:** 2024-11-24
**Maintained By:** Development Team
**Related Documents:**
- `.github/workflows/README.md` - Workflow usage guide
- `CLAUDE.md` - Project development guidelines
- `.claude/docs/technical-spec.md` - Technical specifications
